{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e71bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import os \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "605f9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data_path = os.path.join('..', 'data', \"Louise-Prep-Data-SaraOgMonopolet.json\")\n",
    "df = pd.read_json(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "781c576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log into huggingface:\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # reads .env\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a47ace5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "login() got an unexpected keyword argument 'new_session'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m login\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_session\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: login() got an unexpected keyword argument 'new_session'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ee3c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sæt campingvognen i haven og brug den som klinik/kontor/meditationsrum (og senere evt. gæsteværelse) — det er helt fint, det kan fungere som din virksomhedslokation og kan blive en styrke for dit brand.',\n",
       " 'Fokuser på kvaliteten af din rådgivning og start med at få kunder fra dit nærmeste netværk (venner og bekendte), så positive oplevelser spreder sig mund-til-mund uafhængigt af, at du sidder i en campingvogn.',\n",
       " 'Gør campingvognen pæn og gennemtænkt: vedligehold den, overvej at bygge en lille terrasse foran og sørg for, at den præsenterer sig æstetisk godt, så den ikke fremstår slidt.',\n",
       " 'Brug campingvognen som et anderledes branding-element — fremhæv det som et unikt, mindeværdigt valg (vælg den fremfor et kedeligt kontor i byen) for at tiltrække kunder.',\n",
       " 'Hvis nogen reagerer negativt på campingvognen, forklar at den er midlertidig (sig fx at den kun står der i en periode) som en mulig forklaring eller afværgning.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['acceptable_solutions'].iloc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4689eee0",
   "metadata": {},
   "source": [
    "### Gemma-3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abdf4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify system and user prompts\n",
    "\n",
    "def set_prompts(dilemma):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Du er deltager i et panel, der diskuterer personlige dilemmaer. Du skal give et konkret, utvetydigt svar og kun give mig svaret på dilemmaet og du skal gøre dette med maks 50 tokens, men du skal ikke bare give mig et enkelt ord som svar.\"\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"Her er dilemmaet: {dilemma}\"\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c1fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_prompts(system_prompt, dilemma):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": system_prompt\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"Her er dilemmaet: {dilemma}. Råd: \"\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e240a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Du er deltager i et panel, der diskuterer personlige dilemmaer. Du skal give et konkret, utvetydigt svar og kun give mig svaret på dilemmaet og du skal gøre dette med maks 50 tokens, men du skal ikke bare give mig et enkelt ord som svar.\"\n",
    "new_prompt = \"Du er deltager i et panel, der diskuterer personlige dilemmaer. Giv mig ikke andet end svaret på dilemmaet. Du må ikke kun give et enkelt ord som svar. Det er meget vigtigt, at du kun bruger maks 50 tokens på svaret.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6f2698",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_prompt = \"Du er deltager i et panel, der diskuterer personlige dilemmaer. Du skal give et konkret, utvetydigt svar og kun give mig svaret på dilemmaet. Det er MEGET vigtigt, at du kun skal gøre dette med maks 50 tokens. Du skal heller ikke bare give mig et enkelt ord som svar. Du skal ikke give mig svaret i jeg-person, men blot komme med et råd. Du skal ikke starte med at skrive 'sådan håndterer du det:', men gå lige til rådet. Du skal ikke adressere personen, rådet er til.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e346afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_prompt = \"\"\"Rolle: Du er en ekspert i et panel, der løser personlige dilemmaer med præcision.\n",
    "\n",
    "Opgave: Giv et konkret og utvetydigt råd til det præsenterede dilemma.\n",
    "\n",
    "STRIKSE RETNINGSLINJER:\n",
    "- Format: Gå direkte til rådet uden indledning (undgå f.eks. \"Mit råd er\" eller \"Du bør\").\n",
    "- Stil: Brug neutral, upersonlig stil. Brug ALDRIG ordet \"jeg\" eller \"du\" eller 'dig'.\n",
    "- Længde: MEGET vigtigt det ikke overskrider maks. 80 tokens.\n",
    "- Forbud: Svaret må ikke bestå af kun ét ord.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3169c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8899e8e8b0d74e10ae9bb9d264e12fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading model from huggingface Hub\n",
    "from transformers import AutoProcessor, AutoModelForMultimodalLM\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-12b-it\")\n",
    "model = AutoModelForMultimodalLM.from_pretrained(\"google/gemma-3-12b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "407cf2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'Rolle: Du er en ekspert i et panel, der løser personlige dilemmaer med præcision.\\n\\nOpgave: Giv et konkret og utvetydigt råd til det præsenterede dilemma.\\n\\nStrikte retningslinjer:\\n- Format: Gå direkte til rådet uden indledning (undgå f.eks. \"Mit råd er\" eller \"Du bør\").\\n- Stil: Skriv i neutral eller bydende form. Brug aldrig jeg-person eller direkte henvendelse (som \"du\").\\n- Længde: Svaret skal være kortfattet og præcist (maks. 50 tokens).\\n- Forbud: Svaret må ikke bestå af kun ét ord.\\n'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Her er dilemmaet: På min arbejdsplads har min chef Sanne indført \"Sjov Hattefredag\". Det går simpelt ud på, at alle hver fredag skal møde op iført en sjov hat — det kan være en sombrero, en 12-mands-hat, en badhætte med propeller eller lignende. Min chef har formentlig været på et lederkursus og mener, at det styrker kreativiteten og fællesskabet og giver mulighed for at grine og tale på tværs af afdelinger. Jeg kan godt se, der kan være noget om det, men problemet er, at jeg hader det: jeg synes, det er pinligt at fjolle, og jeg får intet ud af at sidde til møde med en kollega, der har en oppustelig banan på hovedet. Jeg er trods alt 36 år. Jeg har prøvet at holde mig udenom, men føler mig nærmest udskammet på en \"sjov\" måde, hvis jeg ikke tager hatten på — kolleger kommer med kommentarer som \"ej, kom nu. Du kan bare låne min hat, hvis du ikke selv har en\" eller \"man har jo kun det sjove, man selv laver\", leveret med et blink i øjet. Jeg vil ærligt bare gerne arbejde i fred uden at føle mig dum, men jeg frygter, at jeg virker som en kedelig eller sur type, der burde gribe chancen for at skabe et mere uformelt arbejdsmiljø. Kan jeg sige nej tak til de sjove hatte uden at være en dårlig kollega eller skabe dårlig stemning — hvad bør jeg gøre?'}]}]\n"
     ]
    }
   ],
   "source": [
    "messages = set_prompts(optimized_prompt, df['first_person_dilemma'].iloc[10])\n",
    "inputs = processor.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcfde530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forklar over for Sanne, at initiativet skaber ubehag, og at det ikke fremmer arbejdsmiljøet for alle. Anmod om, at det ikke er obligatorisk at deltage.<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens=60)\n",
    "print(processor.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a181f66",
   "metadata": {},
   "source": [
    "### Test Mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7aa2ac76",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vllm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msampling_params\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SamplingParams\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'vllm'"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "from vllm.sampling_params import SamplingParams\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#SYSTEM_PROMPT = \"\"\n",
    "\n",
    "#user_prompt = \"Give me 5 non-formal ways to say 'See you later' in French.\"\n",
    "\n",
    "#messages = [\n",
    "    #{\n",
    "        #\"role\": \"system\",\n",
    "        #\"content\": SYSTEM_PROMPT\n",
    "    #},\n",
    "    #{\n",
    "        #\"role\": \"user\",\n",
    "        #\"content\": user_prompt\n",
    "    #},\n",
    "#]\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "llm = LLM(model=model_name, tokenizer_mode=\"mistral\")\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=60, temperature=0.15)\n",
    "outputs = llm.chat(messages, sampling_params=sampling_params)\n",
    "\n",
    "print(outputs[0].outputs[0].text)\n",
    "# Here are five non-formal ways to say \"See you later\" in French:\n",
    "\n",
    "# 1. **À plus tard** - Until later\n",
    "# 2. **À toute** - See you soon (informal)\n",
    "# 3. **Salut** - Bye (can also mean hi)\n",
    "# 4. **À plus** - See you later (informal)\n",
    "# 5. **Ciao** - Bye (informal, borrowed from Italian)\n",
    "\n",
    "# ```\n",
    "#  /\\_/\\\n",
    "# ( o.o )\n",
    "#  > ^ <\n",
    "# ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12062b8",
   "metadata": {},
   "source": [
    "### Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad65c8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b7f5a687704224ab4f8d5fc7c9bcc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token # we need to specify the pad token specifically\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c44c5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_prompt = \"Du er deltager i et panel, der diskuterer personlige dilemmaer. Du skal give et konkret, utvetydigt svar og kun give mig svaret på dilemmaet. Det er MEGET vigtigt, at du kun skal gøre dette med maks 50 tokens. Du skal heller ikke bare give mig et enkelt ord som svar. Du skal ikke give mig svaret i jeg-person, men blot komme med et råd. Du skal ikke starte med at skrive 'sådan håndterer du det:', men gå lige til rådet.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88e36994",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = \"Du er deltager i et panel, der diskuterer personlige dilemmaer. Giv mig ikke andet end svaret på dilemmaet. Du må ikke kun give et enkelt ord som svar. Det er meget vigtigt, at du kun bruger maks 50 tokens på svaret.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87b24e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_prompt = \"\"\"OPGAVE:\n",
    "Returnér KUN et råd.\n",
    "\n",
    "FORMAT (SKAL OVERHOLDES):\n",
    "- Præcis én sammenhængende tekstblok.\n",
    "- Ingen indledning, ingen afslutning.\n",
    "- Ingen linjeskift.\n",
    "- Ingen anførselstegn.\n",
    "\n",
    "SPROGREGLER (ABSOLUTTE):\n",
    "- INGEN personlige pronomener (fx JEG, DU, DIG, mig, MIN, MIT, mine).\n",
    "- Ingen personlige oplevelser eller fortællinger.\n",
    "- Neutral, upersonlig tone.\n",
    "\n",
    "INDHOLD:\n",
    "- Rådet skal være konkret og handlingsrettet.\n",
    "- Må ikke bestå af kun ét ord.\n",
    "\n",
    "LÆNGDE:\n",
    "- Maksimalt 80 tokens. Stop automatisk ved 80 tokens.\n",
    "\n",
    "FEJL:\n",
    "- Hvis nogen regel brydes, er svaret ugyldigt og skal afkortes.\n",
    "\n",
    "EKSEMPLER PÅ RÅD:\n",
    "\n",
    "- Træde varsomt i kommunikationen med den allernærmeste familie: tage emnet op forsigtigt og vælge sine ord, fordi man skal være påpasselig i nære familiære relationer.\n",
    "- Appellere til klimamæssige og miljømæssige hensyn over for svigerfamilien og bede om brugt legetøj (fx fra genbrugsbutikker) som alternativ til nyt, billigt legetøj\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ba730fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = \"Du er en hjælpsom assistent.\"\n",
    "\n",
    "meta_prompt = f\"Jeg har givet denne prompt til dig, men du kan ikke finde ud af at følge den. Hvad skal jeg ændre på, så du overholder retningslinjerne? Prompten: {gpt_prompt}. hvad skal jeg gøre?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28092d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': [{'type': 'text', 'text': 'Du er en hjælpsom assistent.'}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Her er dilemmaet: Jeg har givet denne prompt til dig, men du kan ikke finde ud af at følge den. Hvad skal jeg ændre på, så du overholder retningslinjerne? Prompten: OPGAVE:\\nReturnér KUN et råd.\\n\\nFORMAT (SKAL OVERHOLDES):\\n- Præcis én sammenhængende tekstblok.\\n- Ingen indledning, ingen afslutning.\\n- Ingen linjeskift.\\n- Ingen anførselstegn.\\n\\nSPROGREGLER (ABSOLUTTE):\\n- INGEN personlige pronomener (fx JEG, DU, DIG, mig, MIN, MIT, mine).\\n- Ingen personlige oplevelser eller fortællinger.\\n- Neutral, upersonlig tone.\\n\\nINDHOLD:\\n- Rådet skal være konkret og handlingsrettet.\\n- Må ikke bestå af kun ét ord.\\n\\nLÆNGDE:\\n- Maksimalt 80 tokens. Stop automatisk ved 80 tokens.\\n\\nFEJL:\\n- Hvis nogen regel brydes, er svaret ugyldigt og skal afkortes.\\n\\nEKSEMPLER PÅ RÅD:\\n\\n- Træde varsomt i kommunikationen med den allernærmeste familie: tage emnet op forsigtigt og vælge sine ord, fordi man skal være påpasselig i nære familiære relationer.\\n- Appellere til klimamæssige og miljømæssige hensyn over for svigerfamilien og bede om brugt legetøj (fx fra genbrugsbutikker) som alternativ til nyt, billigt legetøj\\n\\n. hvad skal jeg gøre?. Råd: '}]}]\n",
      "For at opfylde retningslinjerne, skal du ændre prompten til at være mere præcis og tydelig. Her er nogle forslag:\n",
      "\n",
      "1. Fjern alle eksempler på råd, da de ikke er nødvendige for at forstå hvad der er ønsket.\n",
      "2. Fjern alle reference til fejl, da det ikke er nødvendigt at nævne det.\n",
      "3. Fjern alle reference til længde, da det ikke er nødvendigt at nævne det.\n",
      "4. Fjern alle reference til eksempler på råd, da de ikke er nødvendige for at forstå hvad der er ønsket.\n",
      "5. Fjern alle reference til personlige pronomener, da det er en absolut regel.\n",
      "\n",
      "Her er en forbedret version af prompten:\n",
      "\n",
      "OPGAVE:\n",
      "\n",
      "Returnér KUN et råd.\n",
      "\n",
      "FORMAT (SKAL OVERHOLDES):\n",
      "\n",
      "- Præcis én sammenhængende tekstblok.\n",
      "- Ingen indledning, ingen afslutning.\n",
      "- Ingen linjeskift.\n",
      "- Ingen anførselstegn.\n",
      "\n",
      "SPROGREGLER (ABSOLUTTE):\n",
      "\n",
      "- INGEN personlige pronomener (fx JEG, DU, DIG, mig, MIN, MIT, mine).\n",
      "- Ingen personlige oplevelser eller fortællinger.\n",
      "- Neutral, upersonlig tone.\n",
      "\n",
      "INDHOLD:\n",
      "\n",
      "- Rådet skal være konkret og handlingsrettet.\n",
      "- Må ikke bestå af kun ét ord.\n",
      "\n",
      "FEJL:\n",
      "\n",
      "- Hvis nogen regel brydes, er svaret ugyldigt og skal afkortes.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "#messages = set_prompts(llama_prompt, df['first_person_dilemma'].iloc[1000])\n",
    "messages = set_prompts(meta, meta_prompt)\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "print(messages)\n",
    "outputs = model.generate(\n",
    "\t**inputs, \n",
    "\tmax_new_tokens=1000,\n",
    "\ttemperature=0.3,\n",
    "\tdo_sample=True\n",
    "\t)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a44e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vi skal invitere til børnefødselsdag, men i min svigerfamilie er der en tendens til altid at købe fødselsdagsgaver til vores to små børn — og også til hinandens børn — i Nettos spotvareafdeling. Jeg er simpelthen så træt af at modtage legetøj af så ringe kvalitet, at det går i stykker kort efter. Hjemme går vi generelt op i at købe kvalitet til børnene — både legetøj, tøj og møbler — og vi er alle enige om et budget på 100–150 kroner til hinandens børn, så man sagtens kan finde noget i andre butikker. Nu hvor vi skal holde børnefødselsdag: kan jeg tillade mig at bede gæsterne om ikke at give gaver fra Netto, når min mand mener, at jeg ikke kan?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['first_person_dilemma'].iloc[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c00fd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Sæt et skilt på hoveddøren (fx 'Jeg lufter bare ud' eller 'Do not disturb') så naboer, venner og bekendte forstår, at døren står åben pga. udluftning og ikke er en invitation til at gå ind.\",\n",
       " \"Sig høfligt fra når nogen stikker hovedet ind: 'Det er sødt af dig, men jeg er lige i gang med noget andet – en anden dag.' (gælder overfor venner, bekendte og naboer) — bemærk at det kræver social energi at give afvisningen.\",\n",
       " 'Sæt en fysisk forhindring foran døren (fx en stol eller noget haveaffald) for at signalere, at det ikke er velkomment at træde ind (men vær opmærksom på at folk måske sætter sig og vil snakke).',\n",
       " 'Luk hoveddøren og åbn andre vinduer/døre i lejligheden for at skabe gennemtræk uden at invitere folk ind gennem hoveddøren.',\n",
       " 'Stå i døren og stir på dem uden at sige noget, så de forstår, at hun ikke ønsker besøg (en passiv måde at gøre opmærksom på, at de ikke skal gå ind).',\n",
       " 'Opsæt en rød snor med guldpinde (som en VIP-afspærring) foran døren som en tydelig/legende markering af, at man ikke kan gå ind uden invitation.',\n",
       " \"Brug en billig udgave af 'politiafspærring'-tape foran døren som en tydelig markør for, at der sker noget indenfor og at folk ikke skal komme ind.\",\n",
       " 'Send en sms rundt til venner/bekendte i området og gør dem opmærksomme på, at døren kan stå åben pga. udluftning, men at det ikke er en invitation til at komme ind.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['acceptable_solutions'].iloc[600]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c7730",
   "metadata": {},
   "source": [
    "### Testing batch processing + batch prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5325681",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for i, row in df.iterrows():\n",
    "    prompt = set_prompts(row['first_person_dilemma'])\n",
    "    prompts.append(prompt)\n",
    "\n",
    "df['prompts'] = prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e967e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df to huggingface dataset\n",
    "\n",
    "ds = datasets.Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "56d8d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "\n",
    "# specify tokenizer & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "898a91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_subset = ds.select(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3809b5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column([[{'content': [{'text': \"Du er deltager i et panel, der diskuterer personlige dilemmaer. Du skal give et konkret, utvetydigt svar og kun give mig svaret på dilemmaet. Det er MEGET vigtigt, at du kun skal gøre dette med maks 50 tokens. Du skal heller ikke bare give mig et enkelt ord som svar. Du skal ikke give mig svaret i jeg-person, men blot komme med et råd. Du skal ikke starte med at skrive 'sådan håndterer du det:', men gå lige til rådet. Du skal ikke adressere personen, rådet er til.\", 'type': 'text'}], 'role': 'system'}, {'content': [{'text': \"Her er dilemmaet: Mit navn er Mette, og vores veninde Amalie skal giftes til foråret, så hun skal selvfølgelig have en polterabend. Jeg taler på vegne af alle os, der skal deltage, for vi er havnet i et dilemma. Amalie har givet udtryk over for sin mor, at hun rigtig gerne vil vide, hvornår polterabend'en finder sted. Hun har ondt i maven over, at hun ikke kender datoen, og hun har desuden sagt til sin kommende mand, at hun bliver vred, hvis han ikke fortæller hende datoen. Amalie er ikke så slem, som det lyder — hun er bare lidt af en kontrolfreak, og det har hun helt sikkert sine grunde til. Brudens familie mener bestemt ikke, at hun skal vide det; de mener, det skal være en overraskelse, og at hun i det hele taget vil have godt af at opleve noget, hun ikke kan kontrollere. Hendes mor fortæller for eksempel, at bruden stadig smugkigger i sine julegaver. Brudens veninder er mere i tvivl: vi ved godt, at hun næsten ikke kan holde ud, hvis hun ikke ved, hvad der skal ske. På den ene side synes vi også, det er sjovt at overraske hende, men det synes hun tydeligvis ikke, og vi har egentlig ikke lyst til, at hun skal gå og have ondt i maven over sin egen polterabend. Hvad synes du, vi skal gøre?\", 'type': 'text'}], 'role': 'user'}], [{'content': [{'text': \"Du er deltager i et panel, der diskuterer personlige dilemmaer. Du skal give et konkret, utvetydigt svar og kun give mig svaret på dilemmaet. Det er MEGET vigtigt, at du kun skal gøre dette med maks 50 tokens. Du skal heller ikke bare give mig et enkelt ord som svar. Du skal ikke give mig svaret i jeg-person, men blot komme med et råd. Du skal ikke starte med at skrive 'sådan håndterer du det:', men gå lige til rådet. Du skal ikke adressere personen, rådet er til.\", 'type': 'text'}], 'role': 'system'}, {'content': [{'text': 'Her er dilemmaet: Jeg er på udvekslingsophold i USA og er ved at afslutte mit år. Jeg har boet hos tre forskellige værtsfamilier. I den anden familie havde jeg fire værtsøskende, som jeg alle elsker og har et meget tæt bånd med – blandt dem en dreng, James, på min alder. Fra første gang vi mødtes vidste vi begge, at vores forhold ville udvikle sig til noget mere: vi havde en enorm kemi, og mens jeg boede hos hans familie blomstrede en næsten forbudt forelskelse. De første tre måneder sagde vi ikke noget om vores følelser og benægtede dem, men da James finder ud af, at en af hans venner overvejer at spørge mig med til prom, panikker han og giver mig en lang kærlighedserklæring. Jeg blev ufattelig glad, og pludselig var følelserne på bordet; siden da har vi officielt været kærester, og alt har været fantastisk. Hele forholdet har føltes som en film, og jeg har aldrig følt sådan en forbindelse til nogen anden. Desværre er det uundgåeligt, at jeg meget snart skal rejse hjem til Danmark, og det har vi begge grædt over flere gange. Vi er begge kommet til den erkendelse, at vi ikke kan være sammen efter, jeg er taget hjem, og jeg føler mig forvirret og fortabt over, hvordan vi skal håndtere situationen. Jeg har lovet, at jeg altid vil være en del af hans liv, og at jeg kommer tilbage næste sommer for at se hans familie. Vi vil gerne være venner og holde kontakten, men ved ikke, hvordan vi gør det. Jeg ønsker også, at vi begge får plads til at komme videre og hele. James vil gerne komme til Danmark og møde min familie og se, hvor jeg er vokset op, hvilket på den ene side ville være det fedeste i verden for mig, og på den anden side alt for hårdt og forvirrende. Hvad skal jeg gøre?', 'type': 'text'}], 'role': 'user'}], [{'content': [{'text': \"Du er deltager i et panel, der diskuterer personlige dilemmaer. Du skal give et konkret, utvetydigt svar og kun give mig svaret på dilemmaet. Det er MEGET vigtigt, at du kun skal gøre dette med maks 50 tokens. Du skal heller ikke bare give mig et enkelt ord som svar. Du skal ikke give mig svaret i jeg-person, men blot komme med et råd. Du skal ikke starte med at skrive 'sådan håndterer du det:', men gå lige til rådet. Du skal ikke adressere personen, rådet er til.\", 'type': 'text'}], 'role': 'system'}, {'content': [{'text': 'Her er dilemmaet: Jeg skal snart rejse med min datter til udlandet i en uge, og mit dilemma er akut: jeg er i tvivl om, jeg tør lade min 17-årige søn Lukas være alene hjemme i den periode. Jeg har haft flere planer i spil for ikke at efterlade ham alene, men de er desværre alle gået i vasken. Lukas har altid været lidt vild, risikovillig og uforudsigelig, og derfor er jeg allerede begyndt at forberede mig overdrevent: jeg udarbejder en madplan, skriver huskesedler, googler lokale myndigheder og nødnumre i tilfælde af for eksempel brand eller vandskade, og jeg har arrangeret pasning af vores hund, fordi jeg ved, at Lukas ikke kan styre og tage ansvar for den. Jeg vil gerne være en anti-curling-forælder og give slip og vise tillid, men med en 17-årig er jeg i tvivl om, hvor grænsen går mellem tillid og omsorgssvigt. Jeg ved godt, at sengetider, spisetider og gaming sandsynligvis vil skride, og jeg er indstillet på, at han måske glemmer at låse døren eller lade toiletlåget stå oppe, men mit skrækscenarie er, at han gør noget, der truer husets eller hans egen sikkerhed – at han forårsager brand, vandskade eller lignende. Jeg har et alternativ i ærmet: jeg overvejer at betale naboen for at holde øje i smug, altså holde opsyn uden at Lukas ved det. Kan jeg tillade mig det, eller er jeg en komplet pylret forælder — eller er jeg dybt uansvarlig ved at lade min 17-årige være herre i huset i en uge? Hvad skal jeg gøre?', 'type': 'text'}], 'role': 'user'}], [{'content': [{'text': \"Du er deltager i et panel, der diskuterer personlige dilemmaer. Du skal give et konkret, utvetydigt svar og kun give mig svaret på dilemmaet. Det er MEGET vigtigt, at du kun skal gøre dette med maks 50 tokens. Du skal heller ikke bare give mig et enkelt ord som svar. Du skal ikke give mig svaret i jeg-person, men blot komme med et råd. Du skal ikke starte med at skrive 'sådan håndterer du det:', men gå lige til rådet. Du skal ikke adressere personen, rådet er til.\", 'type': 'text'}], 'role': 'system'}, {'content': [{'text': 'Her er dilemmaet: Jeg er i gang med en uddannelse som klædrådgiver, hvilket i sig selv er meget overvældende, da jeg aldrig havde forestillet mig dette skift i mit arbejdsliv. Jeg bliver færdig til sommer og har fået idéen at stille en campingvogn i haven, hvor jeg kan have min klinik/kontor, fordi der ikke er plads inde i huset, og jeg heller ikke har lyst til at have kunder i mit private hjem. Jeg vil heller ikke have klinik eller kontor et sted i byen, fordi en del af det at blive selvstændig for mig også er, at jeg gerne vil bruge mindre tid på transport. Min mand og jeg har to små børn, og det skal være nemt og lige til at komme på arbejde. Udover at bruge campingvognen som klinik, kontor og meditationsrum, vil den på sigt også kunne bruges som gæsteværelse til overnattende familie og venner — hvem vil ikke gerne sende min snorkende svigerfar ud af huset? Og måske vil vores børn synes, at det er en fest at lege camping ude i haven, altså ren win-win. Men jeg er i tvivl om, hvad potentielle kunder vil tænke: vil det at have en campingvogn i haven som klinik give negative associationer til spåkoner med krystalkugler på markedspladser eller i dårlige film, og gøre mig svær at stole på, i stedet for at blive en del af mit brand og noget nemt at huske, lidt som den fede retro-campingvogn i haven? Jeg er i tvivl om, det i virkeligheden vil holde folk væk, og har brug for andre input end min mands, som går helt i stå over, hvordan campingvognen overhovedet kommer ind i haven — jeg forestiller mig dog, at det kan klares med en kran. Hvad skal jeg gøre?', 'type': 'text'}], 'role': 'user'}], [{'content': [{'text': \"Du er deltager i et panel, der diskuterer personlige dilemmaer. Du skal give et konkret, utvetydigt svar og kun give mig svaret på dilemmaet. Det er MEGET vigtigt, at du kun skal gøre dette med maks 50 tokens. Du skal heller ikke bare give mig et enkelt ord som svar. Du skal ikke give mig svaret i jeg-person, men blot komme med et råd. Du skal ikke starte med at skrive 'sådan håndterer du det:', men gå lige til rådet. Du skal ikke adressere personen, rådet er til.\", 'type': 'text'}], 'role': 'system'}, {'content': [{'text': 'Her er dilemmaet: Sanne og min mand fløj lørdag til Østrig, hvor vi mødtes med et vennepar, og for at vores Tesla ikke skulle stå i lufthavnen hele ugen bad vi vores nabo Benny om at køre os til lufthavnen. Vi har et godt forhold til Benny, hjælper hinanden med stort og småt, og han har tidligere lånt vores sommerhus, så det virkede naturligt at bede ham om et lift. Vi kan følge al kørsel og opladning af vores Tesla på vores telefoner, og søndag morgen fik vi besked om, at opladningen var stoppet, og at bilen var ude at køre — hvilket vi i første omgang troede var en lille tur, vi ikke stillede spørgsmålstegn ved. Over ugen kunne vi dog følge, at Benny dagligt kørte bilen på arbejde; i alt blev der kørt omkring 800 kilometer, og opladningen skete fra vores adresse og ikke fra hans. Altså har han uden aftale brugt både bilen og vores strøm til sin kørsel. Vi har diskuteret det, men er uenige. Jeg mener, det er et tillidsbrud, mens min mand er mere overbærende. Derfor har vi indtil videre kun afbrudt muligheden for opladning fra vores hus, hvilket ikke har afholdt ham fra at køre videre. Nu er vi på vej hjem fra ferie og skal have bilen tilbage i aften, og vi frygter en akavet situation ved gensynet og den efterfølgende hilsen. Skal vi give ham mulighed for selv at erkende, at han har brugt bilen og strømmen — og i så fald hvor længe — eller skal vi konfrontere ham direkte?', 'type': 'text'}], 'role': 'user'}]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_subset['prompts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ab90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "[{'text': \"Du er deltager i et panel, der diskuterer personlige dilemmaer. Du skal give et konkret, utvetydigt svar og kun give mig svaret på dilemmaet. Det er MEGET vigtigt, at du kun skal gøre dette med maks 50 tokens. Du skal heller ikke bare give mig et enkelt ord som svar. Du skal ikke give mig svaret i jeg-person, men blot komme med et råd. Du skal ikke starte med at skrive 'sådan håndterer du det:', men gå lige til rådet. Du skal ikke adressere personen, rådet er til.\", 'type': 'text'}]<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "[{'text': \"Her er dilemmaet: Mit navn er Mette, og vores veninde Amalie skal giftes til foråret, så hun skal selvfølgelig have en polterabend. Jeg taler på vegne af alle os, der skal deltage, for vi er havnet i et dilemma. Amalie har givet udtryk over for sin mor, at hun rigtig gerne vil vide, hvornår polterabend'en finder sted. Hun har ondt i maven over, at hun ikke kender datoen, og hun har desuden sagt til sin kommende mand, at hun bliver vred, hvis han ikke fortæller hende datoen. Amalie er ikke så slem, som det lyder — hun er bare lidt af en kontrolfreak, og det har hun helt sikkert sine grunde til. Brudens familie mener bestemt ikke, at hun skal vide det; de mener, det skal være en overraskelse, og at hun i det hele taget vil have godt af at opleve noget, hun ikke kan kontrollere. Hendes mor fortæller for eksempel, at bruden stadig smugkigger i sine julegaver. Brudens veninder er mere i tvivl: vi ved godt, at hun næsten ikke kan holde ud, hvis hun ikke ved, hvad der skal ske. På den ene side synes vi også, det er sjovt at overraske hende, men det synes hun tydeligvis ikke, og vi har egentlig ikke lyst til, at hun skal gå og have ondt i maven over sin egen polterabend. Hvad synes du, vi skal gøre?\", 'type': 'text'}]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = ds_subset[0][\"prompts\"]\n",
    "\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2999ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_chat(batch):\n",
    "    chat_prompts = [\n",
    "        # convert prompts into a chat template\n",
    "        tokenizer.apply_chat_template( # create chat template used by the loaded model\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False, # not tokenizing yet\n",
    "        )\n",
    "        for messages in batch[\"prompts\"]\n",
    "    ]\n",
    "\n",
    "    # tokenize the chat_template prompts\n",
    "    tokenized_batch = tokenizer(\n",
    "        chat_prompts,\n",
    "        padding=True, # need to add padding for gpu compatability; add padding if sentences do not have the same amount of tokens\n",
    "        truncation=True,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    return tokenized_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b66219b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0bdbeebee7e4250905a49066b39b973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds = ds_subset.map(\n",
    "    tokenize_chat,\n",
    "    batched=True,\n",
    "    remove_columns=ds_subset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59cf7407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from accelerate) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from accelerate) (2.9.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from accelerate) (1.2.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: filelock in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5.1)\n",
      "Requirement already satisfied: anyio in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (4.12.0)\n",
      "Requirement already satisfied: certifi in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.0.9)\n",
      "Requirement already satisfied: idna in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (0.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /work/nlp/monopolet-nlp/.venv/lib/python3.12/site-packages (from typer-slim->huggingface_hub>=0.21.0->accelerate) (8.3.1)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9c26f6",
   "metadata": {},
   "source": [
    "### Test Qwen-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e79c673a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a37892382f48fd9a6e79dedf2f640e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9af032d3da47abb9246652345634e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994d04e3f2fa4155a31f16b4d641ae1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee056b6599834786b0a14e864ce40a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/399 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d551a3804e4515ad578204fda4fce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\"\n",
    "    #device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ebffd744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_prompts_qwen(system_prompt, dilemma):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Her er dilemmaet: {dilemma}. Råd: \"}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84c03aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Du er deltager i et panel, der diskuterer personlige dilemmaer. Du skal give et konkret, utvetydigt svar og kun give mig svaret på dilemmaet. Det er MEGET vigtigt, at du kun skal gøre dette med maks 50 tokens. Du skal heller ikke bare give mig et enkelt ord som svar. Du skal ikke give mig svaret i jeg-person, men blot komme med et råd. Du skal ikke starte med at skrive 'sådan håndterer du det:', men gå lige til rådet.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "401a7ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betale, men sige klart, at du ikke kan fortsætte med at betale, når det fordobles.\n"
     ]
    }
   ],
   "source": [
    "messages = set_prompts_qwen(llama_prompt, df['first_person_dilemma'].iloc[1000])\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=80\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "#try:\n",
    "    # rindex finding 151668 (</think>)\n",
    " #   index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "#except ValueError:\n",
    " #   index = 0\n",
    "\n",
    "#thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[0:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "#print(\"thinking content:\", thinking_content)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a80d424f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jeg skiftede for omtrent et år siden fra det private til det offentlige. I min nye afdeling har man længe haft en morgenmadsordning, hvor man på skift står for at købe ind til fælles morgenmad hver anden uge. Da der er mange i afdelingen, bliver det til mange omgange på et år, og det ender som minimum omkring 1.200 kr. årligt for mig. Derudover er der en gavekasse, som samlet set medfører en årlig udgift på omkring 2.000 kr. Jeg bakker gerne op om det sociale på arbejdspladsen og er en del af en gruppe, som frivilligt arrangerer sociale aktiviteter efter arbejdstid, men jeg mener principielt, at det ikke skal koste så mange penge at være en del af en arbejdsplads. Jeg synes, det er dårligt, at der ikke er afsat penge i afdelingsbudgettet til en månedlig morgenmad, og jeg mener også, at arbejdspladsen burde stå for at sende blomster ved barsel og ved fratrædelser. Dilemmaet er blevet mere aktuelt, fordi der er fremsat forslag om at have morgenmad hver uge, hvilket vil fordoble udgifterne. Jeg har ofte mange møder om ugen, og det betyder for eksempel, at jeg indtil videre kun har kunnet deltage på de dage, hvor jeg selv skulle medbringe morgenmad. Det oplagte svar er måske at melde mig ud af ordningerne, men det er svært, fordi en enkelt kollega aldrig betaler ind til gavekassen, og det skaber sladder i krogene, så jeg overvejer, om jeg skal betale udgifterne for at bevare den gode stemning i afdelingen eller melde mig ud af hensyn til mine principper og mit indre kompas, som har svært ved at sluge, at afdelingens sociale liv skal koste mig penge. Hvad skal jeg gøre?'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['first_person_dilemma'].iloc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89d07870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Stå frem offentligt og sig fra (starte en lille 'revolution') — for eksempel rejse dig op og markere over for kollegerne; gør det charmerende og humoristisk (fx Spartacus-parodi) så du ikke fremstår som pedantisk; involver: Linea og de kolleger, der vil bakke op; betingelse: gør det på en måde, der får andre med.\",\n",
       " 'Medbring din egen morgenmad og spis sammen med kollegerne — deltag socialt uden at betale til morgenmadsordningen; involver: Linea og kollegerne ved fælles morgenmad.',\n",
       " 'Meld dig ud af ordningerne / stop med at betale eller deltage, hvis det strider imod dine principper — vælg at stå udenfor de frivillige ordninger; involver: Linea (konsekvens: mulig sladder/stigmatisering, men legitim mulighed).',\n",
       " 'Fortsæt med at betale for at bevare den gode stemning — vælg at bidrage økonomisk hvis du foretrækker at undgå konflikt og holde ro i afdelingen; involver: Linea.',\n",
       " 'Brug dokumentation (fx et Excel-ark) og tal konkret om økonomien — vis hvor meget du reelt har brugt om året (omkring 1.200 kr.) for at synliggøre problemet og få opbakning; involver: Linea og kollegerne.',\n",
       " 'Gør deltagelse frivillig / foreslå en ordning hvor kun dem, der har lyst, deltager — lad dem, der gerne vil organisere og betale, fortsætte uden at påtvinge andre; involver: Linea, organiserende kolleger og eventuelt ledelsen.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['acceptable_solutions'].iloc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455edee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monopolet_env",
   "language": "python",
   "name": "monopolet_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
