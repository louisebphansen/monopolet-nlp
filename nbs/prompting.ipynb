{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e71bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import os \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import datasets\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb73cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log into huggingface with token to acces llama + gemma models\n",
    "load_dotenv()  # reads .env\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "605f9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data_path = os.path.join('..', 'data', \"monopolet_cleaned.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert the column back to list\n",
    "df['acceptable_solutions_cleaned'] = df['acceptable_solutions_cleaned'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80cfca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt:\n",
    "\n",
    "good_system_prompt = \"\"\"\n",
    "Du er deltager i et panel, der diskuterer personlige dilemmaer. \n",
    "Du skal give et konkret, utvetydigt svar og kun give mig svaret på dilemmaet. \n",
    "Det er MEGET vigtigt, at du kun skal gøre dette med maks 120 tokens. \n",
    "Du skal heller ikke bare give mig et enkelt ord som svar. \n",
    "Du skal ikke give mig svaret i jeg-person, men blot komme med et råd. \n",
    "Du skal ikke starte med at skrive 'sådan håndterer du det:', men gå lige til rådet.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b68fb78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_prompts(system_prompt, dilemma):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": system_prompt\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"Her er dilemmaet: {dilemma}. Råd: \"\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4689eee0",
   "metadata": {},
   "source": [
    "### Gemma-3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02c1fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_prompts(system_prompt, dilemma):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": system_prompt\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"Her er dilemmaet: {dilemma}. Råd: \"\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3169c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67bc957f0cd496eb39031f478701282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5383fa677654159ba61d19faadee3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d670126750f431a9fb82bb5425c65f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cace1d0222443abb394e387e572666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614dda17b7bd4f658cadc797b7ca146b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ab3d998e8c4dfd986b56851bc996a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da222986906540b4a11fa6060ba4e636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072bdb56c0364a829b8f574d344bd935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/916 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a589dfe368a84fa9b6c655edc254e662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/109k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f87e36f0384fa3b384da4c4f900cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf801b0df3514a2abe0c6a9238a06d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543299a2122d433d9fc75cc1603e2448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb7935394da493f90ef4823752e1d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading model from huggingface Hub\n",
    "from transformers import AutoProcessor, AutoModelForMultimodalLM\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-12b-it\")\n",
    "model = AutoModelForMultimodalLM.from_pretrained(\"google/gemma-3-12b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "407cf2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tal med din svigerinde igen. Respekter hendes ønske om ikke at blande dig direkte i hendes forhold, men tilbyd din støtte og lyt uden at dømme. Opmuntre hende til at søge professionel rådgivning for at tackle den økonomiske ubalance og dens indvirkning på hendes trivsel.<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "messages = set_prompts(llama_prompt, df['first_person_dilemma'].iloc[1789])\n",
    "inputs = processor.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=80)\n",
    "print(processor.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0c2f2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Tag en støttende, fortrolig snak med svigerinden alene: Per skal sætte sig ned med sin svigerinde, lytte til hende, lade hende 'tømme sin mentale rygsæk', hjælpe hende reflektere over sine drømme og grænser, og lade hende beslutte, om hun vil blive i forholdet (respektér hvis hun siger 'du må ikke sige noget'). (Deltagere: Per og svigerinden.)\",\n",
       " 'Hjælp svigerinden med at få professionel støtte: foreslå og hjælp hende med at finde en psykolog eller anden fagperson, så hun kan få lagt sit liv og økonomiske følelser på bordet og få klarhed, før der træffes beslutninger om forholdet. (Rækkefølge: først samtale/afklaring med svigerinden, herefter opsøge professionel hjælp. Deltagere: Per, svigerinden og evt. psykolog.)',\n",
       " \"Forsigtigt spørge ind til parrets trivsel uden at moralisere over økonomien: Per kan nænsomt spørge enten svigerinden eller begge parter hvordan de har det, om de er glade, og komme med en lille 'heads up' om, at han oplever en ubalance — men gøre det på en omsorgsfuld måde og undlade at give økonomiske formaninger. (Deltagere: Per, svigerinde og/eller svoger.)\",\n",
       " \"Foreslå (af en af gæsterne som meget klart råd) at svoger må forlade forholdet, hvis han systematisk bruger sin økonomi som psykisk pres: hvis svoger bevidst ydmyger og holder svigerinden tilbage, mente Anders, at han 'skal bare ud' — altså at svigerinden (eller parret) må overveje at afbryde relationen. (Denne løsning blev fremsat som et skarpt råd fra Anders; deltagere: svigerinden og svoger.)\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['acceptable_solutions'].iloc[1789]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a181f66",
   "metadata": {},
   "source": [
    "### Test Mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7aa2ac76",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vllm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msampling_params\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SamplingParams\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'vllm'"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "from vllm.sampling_params import SamplingParams\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#SYSTEM_PROMPT = \"\"\n",
    "\n",
    "#user_prompt = \"Give me 5 non-formal ways to say 'See you later' in French.\"\n",
    "\n",
    "#messages = [\n",
    "    #{\n",
    "        #\"role\": \"system\",\n",
    "        #\"content\": SYSTEM_PROMPT\n",
    "    #},\n",
    "    #{\n",
    "        #\"role\": \"user\",\n",
    "        #\"content\": user_prompt\n",
    "    #},\n",
    "#]\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "llm = LLM(model=model_name, tokenizer_mode=\"mistral\")\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=60, temperature=0.15)\n",
    "outputs = llm.chat(messages, sampling_params=sampling_params)\n",
    "\n",
    "print(outputs[0].outputs[0].text)\n",
    "# Here are five non-formal ways to say \"See you later\" in French:\n",
    "\n",
    "# 1. **À plus tard** - Until later\n",
    "# 2. **À toute** - See you soon (informal)\n",
    "# 3. **Salut** - Bye (can also mean hi)\n",
    "# 4. **À plus** - See you later (informal)\n",
    "# 5. **Ciao** - Bye (informal, borrowed from Italian)\n",
    "\n",
    "# ```\n",
    "#  /\\_/\\\n",
    "# ( o.o )\n",
    "#  > ^ <\n",
    "# ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12062b8",
   "metadata": {},
   "source": [
    "### Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad65c8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856eaf60ff7c4042bc6de49d430f3359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a97565ada04d979d54ad5f1d1eaef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d788087a234e7bb0d19c842058633f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5a074b6ac945b3bcd7f2fd7769eb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841d0ae3fa8a45c2b7e132c9c4ffe80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6620ab98f3854dc3aeaaf8c828f61e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb87dd6775c2406abc65a9bcd2c3bf0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d151c7aa4fd542279f4c3a04df057ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09779a3de9814c21b55204d4457a51dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token # we need to specify the pad token specifically\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b5c3aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_prompt = \"\"\"\n",
    "Du er deltager i et panel, der diskuterer personlige dilemmaer. \n",
    "Du skal give et konkret, utvetydigt svar og kun give mig svaret på dilemmaet. \n",
    "Det er MEGET vigtigt, at du kun skal gøre dette med maks 120 tokens. \n",
    "Du skal heller ikke bare give mig et enkelt ord som svar. \n",
    "Du skal ikke give mig svaret i jeg-person, men blot komme med et råd. \n",
    "Du skal ikke starte med at skrive 'sådan håndterer du det:', men gå lige til rådet.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28092d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': [{'type': 'text', 'text': \"\\nDu er deltager i et panel, der diskuterer personlige dilemmaer. \\nDu skal give et konkret, utvetydigt svar og kun give mig svaret på dilemmaet. \\nDet er MEGET vigtigt, at du kun skal gøre dette med maks 200 tokens. \\nDu skal heller ikke bare give mig et enkelt ord som svar. \\nDu skal ikke give mig svaret i jeg-person, men blot komme med et råd. \\nDu skal ikke starte med at skrive 'sådan håndterer du det:', men gå lige til rådet.\"}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'Her er dilemmaet: Mit navn er Mathias. Jeg har løjet for min kone Molly om, hvad jeg laver efter arbejde. Til daglig har jeg et fysisk krævende job og har fri kl. 14. På det tidspunkt er jeg så træt og drænet, at jeg hver dag kører til en lille, afsidesliggende rasteplads for at tage en morfar på cirka en time. Luren giver mig fornyet energi, og bagefter kører jeg hjem, så jeg rammer hoveddøren kl. 16. Molly tror, jeg har fri kl. 15.30, og hun roser mig ofte for, hvor frisk og energisk jeg er, når jeg kommer hjem. Jeg støvsuger, laver mad, vasker tøj og leger med børnene, og det er den times søvn, der gør det muligt. Molly er hjemme det meste af dagen, fordi hun skriver speciale, og hun har nok at se til med at hente og aflevere børnene hver dag — noget jeg teknisk set godt kunne hjælpe mere med, hvis jeg kom tidligere hjem. Jeg ved dog bare, at hvis jeg dropper min daglige morfar, vil jeg sandsynligvis ikke kunne bidrage på samme måde, som jeg gør nu. Derfor fortsætter jeg med den lille hvide løgn, selvom jeg har dårlig samvittighed, føler mig skyldig og synes, at det næsten føles som en slags utroskab. Jeg oplever, at løgnen i sidste ende gavner hele familien, men er i tvivl om, om jeg skal være ærlig og fortælle Molly sandheden med risiko for at miste det pusterum, der giver mig energi — eller lade stå til og fortsætte med løgnen, som indtil videre gavner familien. Hvad skal jeg gøre?. Råd: '}]}]\n",
      "Fortæl Molly sandheden, men gør det på en måde, der ikke vil skabe konflikt eller skade familien. Først og fremmest skal du forklare, hvorfor du har løjet, og hvordan det har hjulpet familien. Sørg for at være ærlig og åben om dine følelser og bekymringer.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages = set_prompts(llama_prompt, df['first_person_dilemma'].iloc[456])\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "print(messages)\n",
    "outputs = model.generate(\n",
    "\t**inputs, \n",
    "\tmax_new_tokens=200,\n",
    "\ttemperature=0.3,\n",
    "\tdo_sample=True\n",
    "\t)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9053fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 1/6 [01:13<06:08, 73.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 30 output:\n",
      " Sæt en timer på enhederne, så de er automatisk på lydløs efter en vis tid. Dette kan hjælpe din mor med at huske at slå dem på lydløs, uden at du selv skal være involveret i konflikten.<|eot_id|>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [02:59<06:09, 92.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 31 output:\n",
      " Man skal ikke nødvendigvis unfollow de mennesker, man tidligere har været seksuelt sammen med, når man er i et forhold. Det er vigtigt at respektere hinandens grænser og holdninger, men også at være ærlig og åben omkring ens handlinger og følelser. I dette tilfælde kan det være en god ide at have en åben og respektfuld\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [04:14<04:14, 84.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 32 output:\n",
      " Tag kontakt til en af dem på en venlig og undrende måde. Spørg om, hvad der er sket, og hvorfor de ikke blev inviteret. Dette giver dig mulighed for at få en klar forklaring og kan hjælpe med at afklare situationen.<|eot_id|>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [05:27<02:39, 79.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 33 output:\n",
      " Vær ærlig, men præsentér dig selv som en kvinde med en klar vision for fremtiden. Fortæl om dine styrker, dine interesser og dine mål. Fokus på, hvad du kan tilbyde, og hvordan du kan bidrage til en andens liv.<|eot_id|>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [07:04<01:25, 86.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 34 output:\n",
      " Du skal ikke ændre dit efternavn tilbage til dit pigenavn, selv om din eksmand foreslår det. Det er din egen beslutning, og du har allerede sagt nej til det under skilsmissen. Din eksmands nye giftemål skal ikke påvirke din beslutning. Du skal prioritere dine egne følelser og behov, og ikke lade andre bestemme over din identitet.<|eot_id|>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [08:30<00:00, 85.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 35 output:\n",
      " Indfør en begrænsning på antallet af gaver, der gives til børnebørn, f.eks. én gave til jul og én fødselsdagsgave om året. Dette vil hjælpe med at reducere udgifterne og gøre det lettere at håndtere gaveræset.<|eot_id|>\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over rows 30 to 40 (inclusive of 30, exclusive of 41)\n",
    "for i in tqdm(range(30, 36)):\n",
    "    # Create messages for the current row\n",
    "    messages = set_prompts(llama_prompt, df['first_person_dilemma'].iloc[i])\n",
    "    \n",
    "    # Prepare the input for the model\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Print the messages for reference\n",
    "    #print(f\"Row {i} messages:\\n\", messages)\n",
    "    \n",
    "    # Generate model outputs\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.3,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # Decode and print the output\n",
    "    decoded_output = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
    "    print(f\"Row {i} output:\\n\", decoded_output)\n",
    "    print(\"=\"*50)  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0630bb76",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "cacb5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_prompt = \"\"\"Du er en hjælpsom assistent, der skal hjælpe med at bedømme en række udsagn. \n",
    "Giv en score mellem 0-1 på, hvor ens disse to råd er. Du skal give mig et decimaltal.\n",
    "Du kan kun bedømme det på INDHOLDET, og ikke længden, sproget eller andet. \n",
    "Så du skal konkret vurderer, om rådet er det samme i de to eksempler.\n",
    "En score tættere på 0 indikerer, at de rådgiver om helt forskellige ting. \n",
    "En score tættere på 1 indikerer, at de rådgiver om de samme ting.\n",
    "Du skal kun outputte scoren.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d4b24599",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_prompt = \"\"\"Her er rådene, du skal bedømme:\n",
    " 1. Bed din mor om at sætte sin iPhone/iPad på lydløs, når I er sammen, så I kan have en samtale uden baggrundsstøj.\n",
    " 2. Smid din mors telefon ud og sig hun er dum\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d897fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": evaluate_prompt\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": score_prompt\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "76cb7dad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[181]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m inputs = \u001b[43mtokenizer\u001b[49m.apply_chat_template(\n\u001b[32m      2\u001b[39m \tmessages,\n\u001b[32m      3\u001b[39m \tadd_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      4\u001b[39m \ttokenize=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      5\u001b[39m \treturn_dict=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      6\u001b[39m \treturn_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m ).to(model.device)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(messages)\n\u001b[32m     10\u001b[39m outputs = model.generate(\n\u001b[32m     11\u001b[39m \t**inputs, \n\u001b[32m     12\u001b[39m \tmax_new_tokens=\u001b[32m200\u001b[39m,\n\u001b[32m     13\u001b[39m \ttemperature=\u001b[32m0.3\u001b[39m,\n\u001b[32m     14\u001b[39m \tdo_sample=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     15\u001b[39m \t)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "print(messages)\n",
    "outputs = model.generate(\n",
    "\t**inputs, \n",
    "\tmax_new_tokens=200,\n",
    "\ttemperature=0.3,\n",
    "\tdo_sample=True\n",
    "\t)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a59f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nDu er deltager i et panel, der diskuterer personlige dilemmaer. \\nDu skal give et konkret, utvetydigt svar og kun give mig svaret på dilemmaet. \\nDet er MEGET vigtigt, at du kun skal gøre dette med maks 120 tokens. \\nDu skal heller ikke bare give mig et enkelt ord som svar. \\nDu skal ikke give mig svaret i jeg-person, men blot komme med et råd. \\nDu skal ikke starte med at skrive 'sådan håndterer du det:', men gå lige til rådet.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21593ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tag en snak med din mor igen og sig klart, at du gerne vil have hendes fulde opmærksomhed — især når notifikationerne begynder at forstyrre jeres samvær; bed hende lytte til dig.',\n",
       " 'Bed din mor om at sætte sin iPhone/iPad på lydløs, når I er sammen (både hjemme hos hende og ude), så I kan have en samtale uden baggrundsstøj.',\n",
       " 'Vis hende, hvordan man slår notifikationer fra (deaktiverer app‑lyde) og hjælp hende med at slå dem fra, når hun skal være social, og slå dem til igen senere — det kan laves som en hyggelig/legende øvelse mellem jer.',\n",
       " \"Giv hende venligt 'sociale spilleregler' og basic etiquette for sociale medier (takt og tone): forklar, at man ikke underretter hele selskabet hele tiden, og at der er passende måder at være online på i sociale sammenhænge.\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['acceptable_solutions'].iloc[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a44e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mit navn er Mathias. Jeg har løjet for min kone Molly om, hvad jeg laver efter arbejde. Til daglig har jeg et fysisk krævende job og har fri kl. 14. På det tidspunkt er jeg så træt og drænet, at jeg hver dag kører til en lille, afsidesliggende rasteplads for at tage en morfar på cirka en time. Luren giver mig fornyet energi, og bagefter kører jeg hjem, så jeg rammer hoveddøren kl. 16. Molly tror, jeg har fri kl. 15.30, og hun roser mig ofte for, hvor frisk og energisk jeg er, når jeg kommer hjem. Jeg støvsuger, laver mad, vasker tøj og leger med børnene, og det er den times søvn, der gør det muligt. Molly er hjemme det meste af dagen, fordi hun skriver speciale, og hun har nok at se til med at hente og aflevere børnene hver dag — noget jeg teknisk set godt kunne hjælpe mere med, hvis jeg kom tidligere hjem. Jeg ved dog bare, at hvis jeg dropper min daglige morfar, vil jeg sandsynligvis ikke kunne bidrage på samme måde, som jeg gør nu. Derfor fortsætter jeg med den lille hvide løgn, selvom jeg har dårlig samvittighed, føler mig skyldig og synes, at det næsten føles som en slags utroskab. Jeg oplever, at løgnen i sidste ende gavner hele familien, men er i tvivl om, om jeg skal være ærlig og fortælle Molly sandheden med risiko for at miste det pusterum, der giver mig energi — eller lade stå til og fortsætte med løgnen, som indtil videre gavner familien. Hvad skal jeg gøre?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['first_person_dilemma'].iloc[456]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c00fd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fortæl Molly sandheden: sig, at du har løjet om dine eftermiddagslure, forklar at du gør det fordi du har brug for en morfar for at kunne fungere, og forhandl åbent videre — tilbud hende et tilsvarende privilegium (så hun også kan få en time for sig selv) og vær villig til at justere hvor ofte luren tages (fx kun 3 dage om ugen eller andet aftalt antal dage).',\n",
       " \"Sig det på rette tidspunkt og i en god timing: 'jeg skal fortælle dig noget — hver dag imellem X og Y sover jeg, så jeg kan være til stede for jer' — få det ud af systemet, så I kan tale om praktiske løsninger sammen.\",\n",
       " 'Sæt dig ned med dig selv og undersøg, hvorfor du begyndte at lyve (fx konfliktskyhed); betro dig til din ægtefælle om dit behov, og arbejd sammen om at finde måder I begge kan passe på jer selv og hinanden i småbørnsperioden.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['acceptable_solutions'].iloc[456]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c7730",
   "metadata": {},
   "source": [
    "### Testing batch processing + batch prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5325681",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for i, row in df.iterrows():\n",
    "    prompt = set_prompts(good_system_prompt, row['first_person_dilemma'])\n",
    "    prompts.append(prompt)\n",
    "\n",
    "df['prompts'] = prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e967e1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df to huggingface dataset for batch processing\n",
    "ds = datasets.Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56d8d63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f295709cf93b47be9e453f5444f43d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1065 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "# specify tokenizer & model\n",
    "\n",
    "# loading model from huggingface Hub\n",
    "from transformers import AutoProcessor, AutoModelForMultimodalLM\n",
    "\n",
    "tokenizer = AutoProcessor.from_pretrained(\"google/gemma-3-12b-it\")\n",
    "model = AutoModelForMultimodalLM.from_pretrained(\"google/gemma-3-12b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f96302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "# load your tokenizer\n",
    "#tokenizer = AutoProcessor.from_pretrained(\"google/gemma-3-12b-it\")\n",
    "\n",
    "# Example dataframe\n",
    "# df = pd.DataFrame({'acceptable_solutions': [['Hello world', 'This is a test'], ['Another example']]})\n",
    "\n",
    "# Flatten all strings in the column\n",
    "all_strings = [s for lst in df['acceptable_solutions_cleaned'] for s in lst]\n",
    "\n",
    "# Tokenize each string and count tokens\n",
    "token_counts = [len(tokenizer(s).input_ids) for s in all_strings]\n",
    "\n",
    "# Convert to numpy array for easy stats\n",
    "token_counts = np.array(token_counts)\n",
    "\n",
    "# Summary statistics for the entire column (per string)\n",
    "print(\"Number of strings:\", len(token_counts))\n",
    "print(\"Mean tokens per string:\", token_counts.mean())\n",
    "print(\"Min tokens per string:\", token_counts.min())\n",
    "print(\"Max tokens per string:\", token_counts.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "898a91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_subset = ds.select(range(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b7ab90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "\n",
      "Du er deltager i et panel, der diskuterer personlige dilemmaer. \n",
      "Du skal give et konkret, utvetydigt svar og kun give mig svaret på dilemmaet. \n",
      "Det er MEGET vigtigt, at du kun skal gøre dette med maks 120 tokens. \n",
      "Du skal heller ikke bare give mig et enkelt ord som svar. \n",
      "Du skal ikke give mig svaret i jeg-person, men blot komme med et råd. \n",
      "Du skal ikke starte med at skrive 'sådan håndterer du det:', men gå lige til rådet.\n",
      "\n",
      "Her er dilemmaet: Mit navn er Mette, og vores veninde Amalie skal giftes til foråret, så hun skal selvfølgelig have en polterabend. Jeg taler på vegne af alle os, der skal deltage, for vi er havnet i et dilemma. Amalie har givet udtryk over for sin mor, at hun rigtig gerne vil vide, hvornår polterabend'en finder sted. Hun har ondt i maven over, at hun ikke kender datoen, og hun har desuden sagt til sin kommende mand, at hun bliver vred, hvis han ikke fortæller hende datoen. Amalie er ikke så slem, som det lyder — hun er bare lidt af en kontrolfreak, og det har hun helt sikkert sine grunde til. Brudens familie mener bestemt ikke, at hun skal vide det; de mener, det skal være en overraskelse, og at hun i det hele taget vil have godt af at opleve noget, hun ikke kan kontrollere. Hendes mor fortæller for eksempel, at bruden stadig smugkigger i sine julegaver. Brudens veninder er mere i tvivl: vi ved godt, at hun næsten ikke kan holde ud, hvis hun ikke ved, hvad der skal ske. På den ene side synes vi også, det er sjovt at overraske hende, men det synes hun tydeligvis ikke, og vi har egentlig ikke lyst til, at hun skal gå og have ondt i maven over sin egen polterabend. Hvad synes du, vi skal gøre?. Råd:<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print example of prompt\n",
    "messages = ds_subset[0][\"prompts\"]\n",
    "\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8db0a7f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "TokenizersBackend has no attribute tokenizer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokenizer_text = \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m  \u001b[38;5;66;03m# unwrap the text tokenizer\u001b[39;00m\n\u001b[32m      3\u001b[39m token_counts = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lst \u001b[38;5;129;01min\u001b[39;00m df[\u001b[33m'\u001b[39m\u001b[33macceptable_solutions_cleaned\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/monopolet-nlp/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1329\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__getattr__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1326\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convert_tokens_to_ids([\u001b[38;5;28mstr\u001b[39m(tok) \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m tokens]) \u001b[38;5;28;01mif\u001b[39;00m tokens \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m   1328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1331\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattr__\u001b[39m(key)\n",
      "\u001b[31mAttributeError\u001b[39m: TokenizersBackend has no attribute tokenizer"
     ]
    }
   ],
   "source": [
    "tokenizer_text = tokenizer.tokenizer  # unwrap the text tokenizer\n",
    "\n",
    "token_counts = []\n",
    "\n",
    "for lst in df['acceptable_solutions_cleaned']:\n",
    "    for s in lst:\n",
    "        tokens = tokenizer_text.tokenize(s)\n",
    "        token_counts.append(len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f86b180a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mean: 44.804086753898545, min: 2, max: 137'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Mean: {np.mean(token_counts)}, min: {np.min(token_counts)}, max: {np.max(token_counts)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92c75319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mean: 50.670998386807675, min: 3, max: 151'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test same with llama tokenizer;\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "token_counts = []\n",
    "\n",
    "for lst in df['acceptable_solutions_cleaned']:\n",
    "    for s in lst:\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_counts.append(len(tokens))\n",
    "\n",
    "f\"Mean: {np.mean(token_counts)}, min: {np.min(token_counts)}, max: {np.max(token_counts)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32b6e5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(15.277997595515806)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(token_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2999ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_chat(batch):\n",
    "    chat_prompts = [\n",
    "        # convert prompts into a chat template\n",
    "        tokenizer.apply_chat_template( # create chat template used by the loaded model\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False, # not tokenizing yet\n",
    "        )\n",
    "        for messages in batch[\"prompts\"]\n",
    "    ]\n",
    "\n",
    "    # tokenize the chat_template prompts\n",
    "    tokenized_batch = tokenizer(\n",
    "        chat_prompts,\n",
    "        padding=True, # need to add padding for gpu compatability; add padding if sentences do not have the same amount of tokens\n",
    "        truncation=True,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    return tokenized_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b66219b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0bdbeebee7e4250905a49066b39b973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds = ds_subset.map(\n",
    "    tokenize_chat,\n",
    "    batched=True,\n",
    "    remove_columns=ds_subset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9c26f6",
   "metadata": {},
   "source": [
    "### Test Qwen-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e79c673a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a37892382f48fd9a6e79dedf2f640e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9af032d3da47abb9246652345634e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994d04e3f2fa4155a31f16b4d641ae1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee056b6599834786b0a14e864ce40a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/399 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d551a3804e4515ad578204fda4fce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\"\n",
    "    #device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ebffd744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_prompts_qwen(system_prompt, dilemma):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Her er dilemmaet: {dilemma}. Råd: \"}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84c03aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Du er deltager i et panel, der diskuterer personlige dilemmaer. Du skal give et konkret, utvetydigt svar og kun give mig svaret på dilemmaet. Det er MEGET vigtigt, at du kun skal gøre dette med maks 50 tokens. Du skal heller ikke bare give mig et enkelt ord som svar. Du skal ikke give mig svaret i jeg-person, men blot komme med et råd. Du skal ikke starte med at skrive 'sådan håndterer du det:', men gå lige til rådet.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "401a7ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betale, men sige klart, at du ikke kan fortsætte med at betale, når det fordobles.\n"
     ]
    }
   ],
   "source": [
    "messages = set_prompts_qwen(llama_prompt, df['first_person_dilemma'].iloc[1000])\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=80\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "#try:\n",
    "    # rindex finding 151668 (</think>)\n",
    " #   index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "#except ValueError:\n",
    " #   index = 0\n",
    "\n",
    "#thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[0:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "#print(\"thinking content:\", thinking_content)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a80d424f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jeg skiftede for omtrent et år siden fra det private til det offentlige. I min nye afdeling har man længe haft en morgenmadsordning, hvor man på skift står for at købe ind til fælles morgenmad hver anden uge. Da der er mange i afdelingen, bliver det til mange omgange på et år, og det ender som minimum omkring 1.200 kr. årligt for mig. Derudover er der en gavekasse, som samlet set medfører en årlig udgift på omkring 2.000 kr. Jeg bakker gerne op om det sociale på arbejdspladsen og er en del af en gruppe, som frivilligt arrangerer sociale aktiviteter efter arbejdstid, men jeg mener principielt, at det ikke skal koste så mange penge at være en del af en arbejdsplads. Jeg synes, det er dårligt, at der ikke er afsat penge i afdelingsbudgettet til en månedlig morgenmad, og jeg mener også, at arbejdspladsen burde stå for at sende blomster ved barsel og ved fratrædelser. Dilemmaet er blevet mere aktuelt, fordi der er fremsat forslag om at have morgenmad hver uge, hvilket vil fordoble udgifterne. Jeg har ofte mange møder om ugen, og det betyder for eksempel, at jeg indtil videre kun har kunnet deltage på de dage, hvor jeg selv skulle medbringe morgenmad. Det oplagte svar er måske at melde mig ud af ordningerne, men det er svært, fordi en enkelt kollega aldrig betaler ind til gavekassen, og det skaber sladder i krogene, så jeg overvejer, om jeg skal betale udgifterne for at bevare den gode stemning i afdelingen eller melde mig ud af hensyn til mine principper og mit indre kompas, som har svært ved at sluge, at afdelingens sociale liv skal koste mig penge. Hvad skal jeg gøre?'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['first_person_dilemma'].iloc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89d07870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Stå frem offentligt og sig fra (starte en lille 'revolution') — for eksempel rejse dig op og markere over for kollegerne; gør det charmerende og humoristisk (fx Spartacus-parodi) så du ikke fremstår som pedantisk; involver: Linea og de kolleger, der vil bakke op; betingelse: gør det på en måde, der får andre med.\",\n",
       " 'Medbring din egen morgenmad og spis sammen med kollegerne — deltag socialt uden at betale til morgenmadsordningen; involver: Linea og kollegerne ved fælles morgenmad.',\n",
       " 'Meld dig ud af ordningerne / stop med at betale eller deltage, hvis det strider imod dine principper — vælg at stå udenfor de frivillige ordninger; involver: Linea (konsekvens: mulig sladder/stigmatisering, men legitim mulighed).',\n",
       " 'Fortsæt med at betale for at bevare den gode stemning — vælg at bidrage økonomisk hvis du foretrækker at undgå konflikt og holde ro i afdelingen; involver: Linea.',\n",
       " 'Brug dokumentation (fx et Excel-ark) og tal konkret om økonomien — vis hvor meget du reelt har brugt om året (omkring 1.200 kr.) for at synliggøre problemet og få opbakning; involver: Linea og kollegerne.',\n",
       " 'Gør deltagelse frivillig / foreslå en ordning hvor kun dem, der har lyst, deltager — lad dem, der gerne vil organisere og betale, fortsætte uden at påtvinge andre; involver: Linea, organiserende kolleger og eventuelt ledelsen.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['acceptable_solutions'].iloc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455edee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monopolet_env",
   "language": "python",
   "name": "monopolet_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
