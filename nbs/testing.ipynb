{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be723b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/au672746/Library/CloudStorage/OneDrive-Aarhusuniversitet/Skrivebord/monopolet-nlp/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers \n",
    "import bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ef81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9961d8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "advice_one_congr = 'Det bedste vil være at tage en ærlig snak med din leder om, hvor meget du har på tallerkenen lige nu. Forklar, at du gerne vil påtage dig projektet, men at du kun kan gøre det, hvis der bliver prioriteret eller omfordelt nogle af dine nuværende opgaver. På den måde kan I sammen sikre, at kvaliteten af dit arbejde ikke lider.'\n",
    "advice_two_congr = 'Måske kunne det være en god idé først at drøfte din nuværende belastning med din leder. Ved at dele dine bekymringer og behov skaber du mulighed for fælles forventningsafstemning og en løsning, der både tager højde for projektets betydning og din trivsel.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d21a18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 113.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.38 seconds, 2.67 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "P, R, F1 = score(advice_one_congr, advice_two_congr, lang='da', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e03bfcce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6864])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e0a5db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "advice_one_incongr = 'Det kan være hjælpsomt først at tage udgangspunkt i, hvad du realistisk kan rumme lige nu. Hvis projektet presser dig ud over din grænse, kan den mest ansvarlige beslutning være at takke nej, så du bevarer overskud og kvalitet i de opgaver, du allerede håndterer.'\n",
    "advice_two_incongr = 'Du kan også vælge at fokusere på, hvordan projektet kan blive håndterligt, hvis rammerne justeres. I den situation kan du sige ja, men kun hvis du får ekstra støtte, mere tid eller en tydelig prioritering af dine eksisterende opgaver.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae54819b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 558.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.51 seconds, 1.94 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "P_in, R_in, F1_in = score(advice_one_incongr, advice_two_incongr, lang='da', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d16b301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7372])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6384cdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[ 0.0088,  0.1429, -0.0135,  ...,  0.0684, -0.0101, -0.0032],\n",
      "        [ 0.0272,  0.1489, -0.0135,  ...,  0.0822, -0.0594, -0.0096]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = [advice_one_incongr, advice_two_incongr]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "138a35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7762ff12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4f63251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0dc8ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m8/6qm1yrz575b68s5y27_5z0jc33spwq/T/ipykernel_65311/3400734062.py:1: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  cosine_similarity(np.array(sentence_embeddings[0]).reshape(1, -1), np.array(sentence_embeddings[1]).reshape(1, -1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.76294506]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(np.array(sentence_embeddings[0]).reshape(1, -1), np.array(sentence_embeddings[1]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdea908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[-0.0283,  0.1609, -0.0128,  ...,  0.0806, -0.0390, -0.0081],\n",
      "        [-0.0291,  0.1546, -0.0121,  ...,  0.0348, -0.0512,  0.0305]])\n"
     ]
    }
   ],
   "source": [
    "sentences = [advice_one_congr, advice_two_congr]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "#tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "#model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4d9b18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m8/6qm1yrz575b68s5y27_5z0jc33spwq/T/ipykernel_65311/3400734062.py:1: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  cosine_similarity(np.array(sentence_embeddings[0]).reshape(1, -1), np.array(sentence_embeddings[1]).reshape(1, -1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.8297918]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(np.array(sentence_embeddings[0]).reshape(1, -1), np.array(sentence_embeddings[1]).reshape(1, -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monopolet_env",
   "language": "python",
   "name": "monopolet_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
